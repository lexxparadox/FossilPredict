{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d6e2b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten,Conv2D, MaxPooling2D, AveragePooling2D,Activation, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "#from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "import rasterio\n",
    "from rasterio.plot import reshape_as_image\n",
    "\n",
    "#from keras_efficientnets import EfficientNetB0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9cce5c",
   "metadata": {},
   "source": [
    "General Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d92256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple CSV loader\n",
    "def load_samples(csv_file):\n",
    "    data = pd.read_csv(os.path.join(csv_file))\n",
    "    data = data[['FileName','Label','ClassName']]\n",
    "    file_names = list(data.iloc[:,0])\n",
    "    #Get labels withing second column\n",
    "    labels = list(data.iloc[:,1])\n",
    "    samples=[]\n",
    "    for samp,lab in zip(file_names,labels):\n",
    "        samples.append([samp,lab])\n",
    "    return samples\n",
    "\n",
    "#helper function to report  a images details\n",
    "def image_details(image):\n",
    "  num_bands = img_dataset.count\n",
    "  print('Number of bands in image: {n}\\n'.format(n=num_bands))\n",
    "\n",
    "  # How many rows and columns?\n",
    "  rows, cols = img_dataset.shape\n",
    "  print('Image size is: {r} rows x {c} columns\\n'.format(r=rows, c=cols))\n",
    "\n",
    "  # What driver was used to open the raster?\n",
    "  driver = img_dataset.driver\n",
    "  print('Raster driver: {d}\\n'.format(d=driver))\n",
    "\n",
    "  # What is the raster's projection?\n",
    "  proj = img_dataset.crs\n",
    "  print('Image projection:')\n",
    "  print(proj)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, class_dict,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues,model_name='test'):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    # convert class_id to class_name using the class_dict\n",
    "    cover_names = []\n",
    "    for cover_class in classes:\n",
    "        cover_names.append(class_dict[cover_class])\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    else:\n",
    "        pass\n",
    "    #print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=cover_names, yticklabels=cover_names,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    #save a copy of the classification matrix\n",
    "    fig.savefig('classification_matrix_' + model_name + '.png')\n",
    "    return ax\n",
    "\n",
    "def save_metrics(gridsearch_df,test_datagen,test_df,model_name,model,width,height,layers,class_count,test_count,batch,epoch):\n",
    "    #predictions against test dataset\n",
    "    predictions = model.predict_generator(generator=test_datagen, \n",
    "                            steps= test_count // batch,\n",
    "                             verbose=1)\n",
    "\n",
    "    eval_generator = generator(samples=test_df,batch_size=1,width=width,height=height,layers=layers,class_count=class_count)\n",
    "    #determining labels\n",
    "    labels = np.empty(predictions.shape)\n",
    "    count = 0\n",
    "    while count < len(labels):\n",
    "        image_b, label_b = next(eval_generator)\n",
    "        labels[count] = label_b\n",
    "        count += 1\n",
    "\n",
    "    label_index = np.argmax(labels, axis=1)     \n",
    "    pred_index = np.argmax(predictions, axis=1)\n",
    "    #calcualte metrics\n",
    "    accuracy = accuracy_score(label_index, pred_index)\n",
    "    precision = precision_score(label_index, pred_index,average='macro')\n",
    "    recall = recall_score(label_index, pred_index, average='macro')\n",
    "    f1 = f1_score(label_index, pred_index,average='macro')\n",
    "    kappa = cohen_kappa_score(label_index, pred_index)\n",
    "    gridsearch_df=gridsearch_df.append({'Modelname':model_name,'Epoch':epoch,'Batch_size':batch,'Accuracy':accuracy,'Precision':precision,'Recall':recall,'F1_Score':f1,'Cohens_kappa':kappa},ignore_index=True)\n",
    "    #create a classification report and save a copy\n",
    "    clsf_report = pd.DataFrame(classification_report(y_true = label_index, y_pred = pred_index, output_dict=True)).transpose()\n",
    "    clsf_report.to_csv('ClassificationReport_'+ model_name + '.csv', index= True)\n",
    "    \n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    print('Precision: %f' % precision)\n",
    "    print('Recall: %f' % recall)\n",
    "    print('F1 score: %f' % f1)\n",
    "    print('Cohens kappa: %f' % kappa)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plot_confusion_matrix(label_index, pred_index, classes=np.array(list(class_names)),\n",
    "                              class_dict=class_names,model_name=model_name)\n",
    "    \n",
    "    return gridsearch_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3ae586",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e68c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure size of tile is uniform\n",
    "def preprocessing(tile,label,class_count,layers,width,height):\n",
    "    #print(tile.shape,\"before\")\n",
    "    #to avoid artifacts, no antialiasing whn rescaling\n",
    "    tile = resize(tile, (layers,width,height),anti_aliasing=False)\n",
    "    #print(tile.shape,\"resize\")\n",
    "    #Returns the source array reshaped into the order expected by image processing and visualization software (matplotlib, scikit-image, etc) by swapping the axes order from (bands, rows, columns) to (rows, columns, bands)\n",
    "    tile = reshape_as_image(tile)\n",
    "    #print(tile.shape,\"reshape\")\n",
    "    \n",
    "    #normalising tile\n",
    "    tile = tile/255\n",
    "    \n",
    "    label = to_categorical(label,class_count)\n",
    "    return tile,label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c64b1",
   "metadata": {},
   "source": [
    "Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07f113e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras compatible data generator - works with rasterio compatible files (tiff only as written)\n",
    "def generator(samples,batch_size,width,height,layers,class_count):\n",
    "    \"\"\"\n",
    "    Yields next training batch, checks shape of tile, ensure image format - includes DEM\n",
    "    \"\"\"\n",
    "    num_samples = len(samples)\n",
    "    while True: # Loop forever so the generator never terminates\n",
    "        #shuffle(samples)\n",
    "        # Get index to start each batch: [0, batch_size, 2*batch_size, ..., max multiple of batch_size <= num_samples]\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            # Get the samples you'll use in this batch\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            # Initialise X_train and y_train arrays for this batch\n",
    "            X_train = []\n",
    "            y_train = []\n",
    "\n",
    "            # For each example\n",
    "            for batch_sample in batch_samples:\n",
    "                # Load image (X) and label (y)\n",
    "                img_name = batch_sample[0]\n",
    "                label = batch_sample[1]\n",
    "                #load in file\n",
    "                #print(os.path.join(data_path,img_name))\n",
    "                with rasterio.open(os.path.join(img_name)) as ds:\n",
    "                    tile=ds.read()\n",
    "                #perform any preprocessing required\n",
    "                tile,label = preprocessing(tile,label,class_count,layers,width,height)     \n",
    "\n",
    "                # Add example to arrays\n",
    "                X_train.append(tile)\n",
    "                y_train.append(label)\n",
    "\n",
    "            # Make sure they're numpy arrays (as opposed to lists)\n",
    "            X_train = np.array(X_train)\n",
    "            #X_train = np.asarray(X_train).astype('float32')\n",
    "            y_train = np.array(y_train)\n",
    "\n",
    "            # The generator-y part: yield the next training batch            \n",
    "            yield X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a838212e",
   "metadata": {},
   "source": [
    "Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "930c2997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Basic_CNN(input_dropout_rate,hidden_dropout_rate,input_shape,optimizer):\n",
    "    #drop1 = dropout_rate, drop2 = dropout_rate_in\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(input_dropout_rate)) #0.25\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(hidden_dropout_rate)) #0.25\n",
    "\n",
    "    model.add(Dense(class_count))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    #model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=metrics)\n",
    "    #opt = tensorflow.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    #plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19429bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(dropout_rate,input_shape,optimizer):\n",
    "    #drop1 = dropout_rate, drop2 = dropout_rate_in\n",
    "    base_model = applications.resnet50.ResNet50(weights= None, include_top=False, input_shape= input_shape)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    predictions = Dense(class_count, activation= 'softmax')(x)\n",
    "    model = Model(inputs = base_model.input, outputs = predictions)\n",
    "    #model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=metrics)\n",
    "    #opt = tensorflow.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['acc'])\n",
    "    print(model.summary())\n",
    "    print(model.metrics_names)\n",
    "    #plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfbace23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG16(dropout_rate,input_shape,optimizer):\n",
    "    #drop1 = dropout_rate, drop2 = dropout_rate_in\n",
    "    base_model = applications.vgg16.VGG16(weights= None, include_top=False, input_shape= input_shape)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    predictions = Dense(class_count, activation= 'softmax')(x)\n",
    "    model = Model(inputs = base_model.input, outputs = predictions)\n",
    "    #model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=metrics)\n",
    "    #opt = tensorflow.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['acc'])\n",
    "    #print(model.summary())\n",
    "    #print(model.metrics_names)\n",
    "    #plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db30812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Xception(dropout_rate,input_shape,optimizer):\n",
    "    #drop1 = dropout_rate, drop2 = dropout_rate_in\n",
    "    base_model = applications.Xception(weights=None, include_top=False, input_shape= input_shape)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    predictions = Dense(class_count, activation= 'softmax')(x)\n",
    "    model = Model(inputs = base_model.input, outputs = predictions)\n",
    "    #model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=metrics)\n",
    "    #opt = tensorflow.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['acc'])\n",
    "    print(model.summary())\n",
    "    print(model.metrics_names)\n",
    "    #plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "574040ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EfficientNet(dropout_rate,input_shape,optimizer):\n",
    "    #drop1 = dropout_rate, drop2 = dropout_rate_in\n",
    "    base_model = applications.efficientnet.EfficientNetB7(include_top=False,weights=None, input_shape= input_shape)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    predictions = Dense(class_count, activation= 'softmax')(x)\n",
    "    model = Model(inputs = base_model.input, outputs = predictions)\n",
    "    #model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=metrics)\n",
    "    #opt = tensorflow.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['acc'])\n",
    "    #print(model.summary())\n",
    "    #print(model.metrics_names)\n",
    "    #plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfd1165",
   "metadata": {},
   "source": [
    "Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f15d1937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the data list is:  ['noprospect', 'prospect']\n"
     ]
    }
   ],
   "source": [
    "#dictionary necessary to identify classes for confusion matrix\n",
    "class_names = dict((\n",
    "(0,  'noprospect'),\n",
    "(1, 'prospect'),\n",
    "))\n",
    "\n",
    "#class_names = dict((\n",
    "#(0,  'donga'),\n",
    "#(1, 'dried_mudflat'),\n",
    "#(2, 'grass_vegetation'),\n",
    "#(3, 'sandstone'),\n",
    "#(4, 'shrub_vegetation'),\n",
    "#))\n",
    "\n",
    "#Model config\n",
    "width = 32 #32#83  --16\n",
    "height= 32 #32#83  --16\n",
    "layers = 7 #7 sat #6 drone\n",
    "input_shape = (width, height, layers)\n",
    "class_count = 2 #5\n",
    "batch_size = 25 #25\n",
    "epochs = 200 #50\n",
    "learn_rate = 0.001 #0.3\n",
    "momentum = 0.6\n",
    "dropout_rate = 0.5 #0.5\n",
    "dropout_rate2 = 0.25\n",
    "model_name =  'EfficientNetB7_Sat'\n",
    "\n",
    "#declare all needed dataframes\n",
    "gridsearch_df = pd.DataFrame(columns=['Modelname','Epoch','Batch_size','Accuracy','Precision','Recall','F1_Score','Cohens_kappa'])\n",
    "data_path = './SatBinDataset'\n",
    "data_dir_list = os.listdir(data_path)\n",
    "print ('the data list is: ',data_dir_list)\n",
    "fulldataset_df = load_samples('Sat_bin_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fee05c6",
   "metadata": {},
   "source": [
    "Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c490138b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "846\n",
      "6764\n",
      "846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/soft/sl7/gen5-gpu/intel/2020/intelpython3/gen5-gpu/envs/tensorflow_2.7.0/lib/python3.7/site-packages/keras/applications/efficientnet.py:294: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels.\n",
      "  weights=weights)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 8.3378 - acc: 0.5818\n",
      "Epoch 00001: val_loss improved from inf to 242.18544, saving model to model-inprogress-multi.h5\n",
      "270/270 [==============================] - 68s 151ms/step - loss: 8.3378 - acc: 0.5818 - val_loss: 242.1854 - val_acc: 0.6182 - lr: 0.1000\n",
      "Epoch 2/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 9.5648 - acc: 0.5346\n",
      "Epoch 00002: val_loss did not improve from 242.18544\n",
      "270/270 [==============================] - 36s 131ms/step - loss: 9.5648 - acc: 0.5346 - val_loss: 68058320.0000 - val_acc: 0.5006 - lr: 0.1000\n",
      "Epoch 3/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 8.0560 - acc: 0.4918\n",
      "Epoch 00003: val_loss improved from 242.18544 to 9.18021, saving model to model-inprogress-multi.h5\n",
      "270/270 [==============================] - 37s 135ms/step - loss: 8.0560 - acc: 0.4918 - val_loss: 9.1802 - val_acc: 0.4994 - lr: 0.1000\n",
      "Epoch 4/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 4.8749 - acc: 0.5017\n",
      "Epoch 00004: val_loss improved from 9.18021 to 4.28517, saving model to model-inprogress-multi.h5\n",
      "270/270 [==============================] - 36s 134ms/step - loss: 4.8749 - acc: 0.5017 - val_loss: 4.2852 - val_acc: 0.4994 - lr: 0.1000\n",
      "Epoch 5/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 4.0499 - acc: 0.4944\n",
      "Epoch 00005: val_loss improved from 4.28517 to 1.07515, saving model to model-inprogress-multi.h5\n",
      "270/270 [==============================] - 36s 135ms/step - loss: 4.0499 - acc: 0.4944 - val_loss: 1.0751 - val_acc: 0.4994 - lr: 0.1000\n",
      "Epoch 6/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 2.9386 - acc: 0.5486\n",
      "Epoch 00006: val_loss did not improve from 1.07515\n",
      "270/270 [==============================] - 35s 130ms/step - loss: 2.9386 - acc: 0.5486 - val_loss: 2.7990 - val_acc: 0.5006 - lr: 0.1000\n",
      "Epoch 7/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 2.3389 - acc: 0.5645\n",
      "Epoch 00007: val_loss did not improve from 1.07515\n",
      "270/270 [==============================] - 35s 130ms/step - loss: 2.3389 - acc: 0.5645 - val_loss: 2.1268 - val_acc: 0.4994 - lr: 0.1000\n",
      "Epoch 8/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 2.0576 - acc: 0.4968\n",
      "Epoch 00008: val_loss improved from 1.07515 to 0.74046, saving model to model-inprogress-multi.h5\n",
      "270/270 [==============================] - 36s 134ms/step - loss: 2.0576 - acc: 0.4968 - val_loss: 0.7405 - val_acc: 0.5006 - lr: 0.1000\n",
      "Epoch 9/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 2.2560 - acc: 0.5068\n",
      "Epoch 00009: val_loss improved from 0.74046 to 0.67897, saving model to model-inprogress-multi.h5\n",
      "270/270 [==============================] - 39s 144ms/step - loss: 2.2560 - acc: 0.5068 - val_loss: 0.6790 - val_acc: 0.5709 - lr: 0.1000\n",
      "Epoch 10/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 2.1322 - acc: 0.5047\n",
      "Epoch 00010: val_loss did not improve from 0.67897\n",
      "270/270 [==============================] - 35s 131ms/step - loss: 2.1322 - acc: 0.5047 - val_loss: 56.0298 - val_acc: 0.4836 - lr: 0.1000\n",
      "Epoch 11/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 2.0037 - acc: 0.5026\n",
      "Epoch 00011: val_loss did not improve from 0.67897\n",
      "270/270 [==============================] - 35s 131ms/step - loss: 2.0037 - acc: 0.5026 - val_loss: 1.1391 - val_acc: 0.4994 - lr: 0.1000\n",
      "Epoch 12/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 3.9969 - acc: 0.5024\n",
      "Epoch 00012: val_loss did not improve from 0.67897\n",
      "270/270 [==============================] - 35s 131ms/step - loss: 3.9969 - acc: 0.5024 - val_loss: 22850162.0000 - val_acc: 0.3115 - lr: 0.1000\n",
      "Epoch 13/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 2.2397 - acc: 0.5026\n",
      "Epoch 00013: val_loss did not improve from 0.67897\n",
      "270/270 [==============================] - 35s 130ms/step - loss: 2.2397 - acc: 0.5026 - val_loss: 0.7000 - val_acc: 0.5042 - lr: 0.1000\n",
      "Epoch 14/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 1.1510 - acc: 0.5017\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.67897\n",
      "270/270 [==============================] - 35s 130ms/step - loss: 1.1510 - acc: 0.5017 - val_loss: 0.6829 - val_acc: 0.4958 - lr: 0.1000\n",
      "Epoch 15/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.8510 - acc: 0.5151\n",
      "Epoch 00015: val_loss did not improve from 0.67897\n",
      "270/270 [==============================] - 35s 131ms/step - loss: 0.8510 - acc: 0.5151 - val_loss: 0.7276 - val_acc: 0.5006 - lr: 0.0100\n",
      "Epoch 16/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.7693 - acc: 0.5090\n",
      "Epoch 00016: val_loss did not improve from 0.67897\n",
      "270/270 [==============================] - 35s 130ms/step - loss: 0.7693 - acc: 0.5090 - val_loss: 0.7084 - val_acc: 0.4994 - lr: 0.0100\n",
      "Epoch 17/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.7534 - acc: 0.5029\n",
      "Epoch 00017: val_loss did not improve from 0.67897\n",
      "270/270 [==============================] - 36s 134ms/step - loss: 0.7534 - acc: 0.5029 - val_loss: 0.7040 - val_acc: 0.4994 - lr: 0.0100\n",
      "Epoch 18/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.7379 - acc: 0.5039\n",
      "Epoch 00018: val_loss did not improve from 0.67897\n",
      "270/270 [==============================] - 36s 132ms/step - loss: 0.7379 - acc: 0.5039 - val_loss: 0.7016 - val_acc: 0.4994 - lr: 0.0100\n",
      "Epoch 19/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.7254 - acc: 0.5062\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.67897\n",
      "270/270 [==============================] - 35s 131ms/step - loss: 0.7254 - acc: 0.5062 - val_loss: 0.6956 - val_acc: 0.4994 - lr: 0.0100\n",
      "Epoch 00019: early stopping\n",
      "INFO:tensorflow:Assets written to: EfficientNetB7_Sat_5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/soft/sl7/gen5-gpu/intel/2020/intelpython3/gen5-gpu/envs/tensorflow_2.7.0/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/soft/sl7/gen5-gpu/intel/2020/intelpython3/gen5-gpu/envs/tensorflow_2.7.0/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n",
      "/soft/sl7/gen5-gpu/intel/2020/intelpython3/gen5-gpu/envs/tensorflow_2.7.0/lib/python3.7/site-packages/ipykernel_launcher.py:92: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 6s 72ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dollmangj/.conda/gen5-gpu/intel2020/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dollmangj/.conda/gen5-gpu/intel2020/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/soft/sl7/gen5-gpu/intel/2020/intelpython3/gen5-gpu/envs/tensorflow_2.7.0/lib/python3.7/site-packages/keras/applications/efficientnet.py:294: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels.\n",
      "  weights=weights)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.484848\n",
      "Precision: 0.242424\n",
      "Recall: 0.500000\n",
      "F1 score: 0.326531\n",
      "Cohens kappa: 0.000000\n",
      "Epoch 1/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 1.1405 - acc: 0.5855\n",
      "Epoch 00001: val_loss did not improve from 0.67897\n",
      "270/270 [==============================] - 67s 148ms/step - loss: 1.1405 - acc: 0.5855 - val_loss: 0.7110 - val_acc: 0.5006 - lr: 1.0000e-03\n",
      "Epoch 2/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.6019 - acc: 0.8050\n",
      "Epoch 00002: val_loss improved from 0.67897 to 0.65288, saving model to model-inprogress-multi.h5\n",
      "270/270 [==============================] - 37s 138ms/step - loss: 0.6019 - acc: 0.8050 - val_loss: 0.6529 - val_acc: 0.6364 - lr: 1.0000e-03\n",
      "Epoch 3/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.4886 - acc: 0.8489\n",
      "Epoch 00003: val_loss improved from 0.65288 to 0.35638, saving model to model-inprogress-multi.h5\n",
      "270/270 [==============================] - 42s 155ms/step - loss: 0.4886 - acc: 0.8489 - val_loss: 0.3564 - val_acc: 0.8630 - lr: 1.0000e-03\n",
      "Epoch 4/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.2961 - acc: 0.8837\n",
      "Epoch 00004: val_loss improved from 0.35638 to 0.24097, saving model to model-inprogress-multi.h5\n",
      "270/270 [==============================] - 38s 140ms/step - loss: 0.2961 - acc: 0.8837 - val_loss: 0.2410 - val_acc: 0.8861 - lr: 1.0000e-03\n",
      "Epoch 5/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.2367 - acc: 0.9044\n",
      "Epoch 00005: val_loss improved from 0.24097 to 0.14841, saving model to model-inprogress-multi.h5\n",
      "270/270 [==============================] - 37s 139ms/step - loss: 0.2367 - acc: 0.9044 - val_loss: 0.1484 - val_acc: 0.9552 - lr: 1.0000e-03\n",
      "Epoch 6/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.1985 - acc: 0.9202\n",
      "Epoch 00006: val_loss improved from 0.14841 to 0.12244, saving model to model-inprogress-multi.h5\n",
      "270/270 [==============================] - 37s 137ms/step - loss: 0.1985 - acc: 0.9202 - val_loss: 0.1224 - val_acc: 0.9576 - lr: 1.0000e-03\n",
      "Epoch 7/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.1890 - acc: 0.9273\n",
      "Epoch 00007: val_loss improved from 0.12244 to 0.11576, saving model to model-inprogress-multi.h5\n",
      "270/270 [==============================] - 37s 136ms/step - loss: 0.1890 - acc: 0.9273 - val_loss: 0.1158 - val_acc: 0.9515 - lr: 1.0000e-03\n",
      "Epoch 8/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.6275 - acc: 0.7568\n",
      "Epoch 00008: val_loss did not improve from 0.11576\n",
      "270/270 [==============================] - 35s 130ms/step - loss: 0.6275 - acc: 0.7568 - val_loss: 0.8486 - val_acc: 0.6764 - lr: 1.0000e-03\n",
      "Epoch 9/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.6565 - acc: 0.6675\n",
      "Epoch 00009: val_loss did not improve from 0.11576\n",
      "270/270 [==============================] - 35s 130ms/step - loss: 0.6565 - acc: 0.6675 - val_loss: 1.3134 - val_acc: 0.8109 - lr: 1.0000e-03\n",
      "Epoch 10/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.4427 - acc: 0.8129\n",
      "Epoch 00010: val_loss did not improve from 0.11576\n",
      "270/270 [==============================] - 35s 130ms/step - loss: 0.4427 - acc: 0.8129 - val_loss: 0.4342 - val_acc: 0.9127 - lr: 1.0000e-03\n",
      "Epoch 11/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.2865 - acc: 0.8970\n",
      "Epoch 00011: val_loss did not improve from 0.11576\n",
      "270/270 [==============================] - 38s 140ms/step - loss: 0.2865 - acc: 0.8970 - val_loss: 0.3091 - val_acc: 0.9418 - lr: 1.0000e-03\n",
      "Epoch 12/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.2438 - acc: 0.9193\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.11576\n",
      "270/270 [==============================] - 39s 143ms/step - loss: 0.2438 - acc: 0.9193 - val_loss: 0.2207 - val_acc: 0.9079 - lr: 1.0000e-03\n",
      "Epoch 13/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.1927 - acc: 0.9389\n",
      "Epoch 00013: val_loss did not improve from 0.11576\n",
      "270/270 [==============================] - 37s 137ms/step - loss: 0.1927 - acc: 0.9389 - val_loss: 0.1317 - val_acc: 0.9673 - lr: 1.0000e-04\n",
      "Epoch 14/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.1719 - acc: 0.9463\n",
      "Epoch 00014: val_loss did not improve from 0.11576\n",
      "270/270 [==============================] - 36s 135ms/step - loss: 0.1719 - acc: 0.9463 - val_loss: 0.1251 - val_acc: 0.9697 - lr: 1.0000e-04\n",
      "Epoch 15/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.1670 - acc: 0.9436\n",
      "Epoch 00015: val_loss did not improve from 0.11576\n",
      "270/270 [==============================] - 36s 134ms/step - loss: 0.1670 - acc: 0.9436 - val_loss: 0.1195 - val_acc: 0.9709 - lr: 1.0000e-04\n",
      "Epoch 16/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.1596 - acc: 0.9478\n",
      "Epoch 00016: val_loss did not improve from 0.11576\n",
      "270/270 [==============================] - 36s 135ms/step - loss: 0.1596 - acc: 0.9478 - val_loss: 0.1185 - val_acc: 0.9709 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.1409 - acc: 0.9522\n",
      "Epoch 00017: val_loss improved from 0.11576 to 0.11091, saving model to model-inprogress-multi.h5\n",
      "270/270 [==============================] - 38s 140ms/step - loss: 0.1409 - acc: 0.9522 - val_loss: 0.1109 - val_acc: 0.9745 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.1438 - acc: 0.9544\n",
      "Epoch 00018: val_loss improved from 0.11091 to 0.10819, saving model to model-inprogress-multi.h5\n",
      "270/270 [==============================] - 38s 140ms/step - loss: 0.1438 - acc: 0.9544 - val_loss: 0.1082 - val_acc: 0.9721 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.1301 - acc: 0.9592\n",
      "Epoch 00019: val_loss improved from 0.10819 to 0.10173, saving model to model-inprogress-multi.h5\n",
      "270/270 [==============================] - 39s 144ms/step - loss: 0.1301 - acc: 0.9592 - val_loss: 0.1017 - val_acc: 0.9745 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.1235 - acc: 0.9595\n",
      "Epoch 00020: val_loss improved from 0.10173 to 0.09561, saving model to model-inprogress-multi.h5\n",
      "270/270 [==============================] - 40s 148ms/step - loss: 0.1235 - acc: 0.9595 - val_loss: 0.0956 - val_acc: 0.9794 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.1083 - acc: 0.9659\n",
      "Epoch 00021: val_loss improved from 0.09561 to 0.09452, saving model to model-inprogress-multi.h5\n",
      "270/270 [==============================] - 38s 142ms/step - loss: 0.1083 - acc: 0.9659 - val_loss: 0.0945 - val_acc: 0.9745 - lr: 1.0000e-04\n",
      "Epoch 22/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0982 - acc: 0.9677\n",
      "Epoch 00022: val_loss did not improve from 0.09452\n",
      "270/270 [==============================] - 36s 134ms/step - loss: 0.0982 - acc: 0.9677 - val_loss: 0.0966 - val_acc: 0.9745 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0989 - acc: 0.9712\n",
      "Epoch 00023: val_loss improved from 0.09452 to 0.08656, saving model to model-inprogress-multi.h5\n",
      "270/270 [==============================] - 38s 139ms/step - loss: 0.0989 - acc: 0.9712 - val_loss: 0.0866 - val_acc: 0.9806 - lr: 1.0000e-04\n",
      "Epoch 24/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0819 - acc: 0.9727\n",
      "Epoch 00024: val_loss improved from 0.08656 to 0.07586, saving model to model-inprogress-multi.h5\n",
      "270/270 [==============================] - 37s 139ms/step - loss: 0.0819 - acc: 0.9727 - val_loss: 0.0759 - val_acc: 0.9758 - lr: 1.0000e-04\n",
      "Epoch 25/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0809 - acc: 0.9725\n",
      "Epoch 00025: val_loss did not improve from 0.07586\n",
      "270/270 [==============================] - 36s 134ms/step - loss: 0.0809 - acc: 0.9725 - val_loss: 0.0770 - val_acc: 0.9745 - lr: 1.0000e-04\n",
      "Epoch 26/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0696 - acc: 0.9792\n",
      "Epoch 00026: val_loss improved from 0.07586 to 0.07010, saving model to model-inprogress-multi.h5\n",
      "270/270 [==============================] - 37s 139ms/step - loss: 0.0696 - acc: 0.9792 - val_loss: 0.0701 - val_acc: 0.9770 - lr: 1.0000e-04\n",
      "Epoch 27/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0692 - acc: 0.9766\n",
      "Epoch 00027: val_loss did not improve from 0.07010\n",
      "270/270 [==============================] - 37s 137ms/step - loss: 0.0692 - acc: 0.9766 - val_loss: 0.0959 - val_acc: 0.9673 - lr: 1.0000e-04\n",
      "Epoch 28/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0704 - acc: 0.9776\n",
      "Epoch 00028: val_loss did not improve from 0.07010\n",
      "270/270 [==============================] - 39s 143ms/step - loss: 0.0704 - acc: 0.9776 - val_loss: 0.0885 - val_acc: 0.9673 - lr: 1.0000e-04\n",
      "Epoch 29/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0508 - acc: 0.9852\n",
      "Epoch 00029: val_loss did not improve from 0.07010\n",
      "270/270 [==============================] - 37s 136ms/step - loss: 0.0508 - acc: 0.9852 - val_loss: 0.1217 - val_acc: 0.9539 - lr: 1.0000e-04\n",
      "Epoch 30/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0628 - acc: 0.9803\n",
      "Epoch 00030: val_loss did not improve from 0.07010\n",
      "270/270 [==============================] - 37s 136ms/step - loss: 0.0628 - acc: 0.9803 - val_loss: 0.0730 - val_acc: 0.9758 - lr: 1.0000e-04\n",
      "Epoch 31/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0493 - acc: 0.9844\n",
      "Epoch 00031: val_loss improved from 0.07010 to 0.06967, saving model to model-inprogress-multi.h5\n",
      "270/270 [==============================] - 38s 139ms/step - loss: 0.0493 - acc: 0.9844 - val_loss: 0.0697 - val_acc: 0.9733 - lr: 1.0000e-04\n",
      "Epoch 32/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0489 - acc: 0.9871\n",
      "Epoch 00032: val_loss did not improve from 0.06967\n",
      "270/270 [==============================] - 36s 133ms/step - loss: 0.0489 - acc: 0.9871 - val_loss: 0.0710 - val_acc: 0.9697 - lr: 1.0000e-04\n",
      "Epoch 33/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0446 - acc: 0.9896\n",
      "Epoch 00033: val_loss did not improve from 0.06967\n",
      "270/270 [==============================] - 35s 131ms/step - loss: 0.0446 - acc: 0.9896 - val_loss: 0.0771 - val_acc: 0.9697 - lr: 1.0000e-04\n",
      "Epoch 34/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0450 - acc: 0.9840\n",
      "Epoch 00034: val_loss did not improve from 0.06967\n",
      "270/270 [==============================] - 35s 130ms/step - loss: 0.0450 - acc: 0.9840 - val_loss: 0.0927 - val_acc: 0.9697 - lr: 1.0000e-04\n",
      "Epoch 35/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0356 - acc: 0.9893\n",
      "Epoch 00035: val_loss improved from 0.06967 to 0.06438, saving model to model-inprogress-multi.h5\n",
      "270/270 [==============================] - 36s 135ms/step - loss: 0.0356 - acc: 0.9893 - val_loss: 0.0644 - val_acc: 0.9782 - lr: 1.0000e-04\n",
      "Epoch 36/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0246 - acc: 0.9915\n",
      "Epoch 00036: val_loss did not improve from 0.06438\n",
      "270/270 [==============================] - 39s 144ms/step - loss: 0.0246 - acc: 0.9915 - val_loss: 0.0822 - val_acc: 0.9745 - lr: 1.0000e-04\n",
      "Epoch 37/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0329 - acc: 0.9902\n",
      "Epoch 00037: val_loss did not improve from 0.06438\n",
      "270/270 [==============================] - 37s 136ms/step - loss: 0.0329 - acc: 0.9902 - val_loss: 0.0709 - val_acc: 0.9709 - lr: 1.0000e-04\n",
      "Epoch 38/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0294 - acc: 0.9909\n",
      "Epoch 00038: val_loss did not improve from 0.06438\n",
      "270/270 [==============================] - 36s 135ms/step - loss: 0.0294 - acc: 0.9909 - val_loss: 0.2492 - val_acc: 0.9127 - lr: 1.0000e-04\n",
      "Epoch 39/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0307 - acc: 0.9911\n",
      "Epoch 00039: val_loss did not improve from 0.06438\n",
      "270/270 [==============================] - 36s 134ms/step - loss: 0.0307 - acc: 0.9911 - val_loss: 0.2651 - val_acc: 0.9030 - lr: 1.0000e-04\n",
      "Epoch 40/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0434 - acc: 0.9869\n",
      "Epoch 00040: val_loss improved from 0.06438 to 0.05425, saving model to model-inprogress-multi.h5\n",
      "270/270 [==============================] - 38s 140ms/step - loss: 0.0434 - acc: 0.9869 - val_loss: 0.0542 - val_acc: 0.9818 - lr: 1.0000e-04\n",
      "Epoch 41/200\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0313 - acc: 0.9904\n",
      "Epoch 00041: val_loss did not improve from 0.05425\n",
      "270/270 [==============================] - 36s 134ms/step - loss: 0.0313 - acc: 0.9904 - val_loss: 0.1033 - val_acc: 0.9709 - lr: 1.0000e-04\n",
      "Epoch 42/200\n",
      "104/270 [==========>...................] - ETA: 20s - loss: 0.0251 - acc: 0.9919"
     ]
    }
   ],
   "source": [
    "train_df,test_df = train_test_split(fulldataset_df, test_size=0.2) #0.2\n",
    "test_df,valid_df = train_test_split(test_df, test_size=0.5)\n",
    "\n",
    "\n",
    "\n",
    "#counts necessary\n",
    "valid_count = len(valid_df)\n",
    "train_count = len(train_df)\n",
    "test_count = len(test_df)\n",
    "print(valid_count)\n",
    "print(train_count)\n",
    "print(test_count)\n",
    "\n",
    "#optimizer = Ftrl(lr=learn_rates)\n",
    "#optimizer = SGD(lr=learn_rate,momentum=momentum)\n",
    "optimizer = Adam(learning_rate=0.1, epsilon=1e-07) #1.0 or 0.1\n",
    "\n",
    "count = 5\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=10, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint('model-inprogress-multi.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "]\n",
    "\n",
    "while count > 0:\n",
    "    smodel_name =  model_name +\"_\" + str(count)\n",
    "    count -= 1\n",
    "    input_shape = (width, height, layers)\n",
    "    \n",
    "    #essential to create a fresh model every time\n",
    "    #Create the generators \n",
    "    train_datagen = generator(samples=train_df,batch_size=batch_size,width=width,height=height,layers=layers,class_count=class_count)\n",
    "    test_datagen = generator(samples=test_df,batch_size=batch_size,width=width,height=height,layers=layers,class_count=class_count)\n",
    "    valid_datagen = generator(samples=valid_df,batch_size=batch_size,width=width,height=height,layers=layers,class_count=class_count)\n",
    "\n",
    "    #create and laod model\n",
    "    model = EfficientNet(dropout_rate,input_shape,optimizer)\n",
    "    #model = Basic_CNN(dropout_rate,dropout_rate2,input_shape,optimizer)\n",
    "\n",
    "    #create_model_options(input_dropout_rate,hidden_dropout_rate,input_shape,optimizer)\n",
    "    history = model.fit(train_datagen,steps_per_epoch=train_count // batch_size, verbose=1, epochs=epochs,validation_data=valid_datagen,validation_steps=valid_count // batch_size,callbacks=callbacks)\n",
    "    #steps_per_epoch: Total number of steps (batches of samples) to yield from generator before declaring one epoch finished and starting the next epoch. It should typically be equal to the number of unique samples of your dataset divided by the batch size.\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    #plt.plot(history.history['acc']) #anything but sequential models\n",
    "    #plt.plot(history.history['val_acc'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train_Accuracy', 'Valid_Accuracy'], loc='upper left')\n",
    "    plt.savefig('accuracy_' + smodel_name + '.png', dpi=300)\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train_Loss', 'Valid_Loss'], loc='upper left')\n",
    "    plt.savefig('loss_' + smodel_name + '.png',dpi=300)\n",
    "    \n",
    "    #save model for later use\n",
    "    model.save(smodel_name)\n",
    "    \n",
    "    #run the metrics and save it to a dataframe for later analysis\n",
    "    gridsearch_df = save_metrics(gridsearch_df,test_datagen,test_df,smodel_name,model,width,height,layers,class_count,test_count,batch_size,epochs)\n",
    "    \n",
    "    #save results\n",
    "    gridsearch_df.to_csv('Total_Results_'+ smodel_name + '.csv', index= True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ab6c90",
   "metadata": {},
   "source": [
    "Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b193b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple CSV loader\n",
    "def load_samples(csv_file):\n",
    "    data = pd.read_csv(os.path.join(csv_file))\n",
    "    data = data[['FileName','Label','ClassName']]\n",
    "    file_names = list(data.iloc[:,0])\n",
    "    #Get labels withing second column\n",
    "    labels = list(data.iloc[:,1])\n",
    "    samples=[]\n",
    "    for samp,lab in zip(file_names,labels):\n",
    "        samples.append([samp,lab])\n",
    "    return samples\n",
    "\n",
    "#helper function to report  a images details\n",
    "def image_details(image):\n",
    "  num_bands = img_dataset.count\n",
    "  print('Number of bands in image: {n}\\n'.format(n=num_bands))\n",
    "\n",
    "  # How many rows and columns?\n",
    "  rows, cols = img_dataset.shape\n",
    "  print('Image size is: {r} rows x {c} columns\\n'.format(r=rows, c=cols))\n",
    "\n",
    "  # What driver was used to open the raster?\n",
    "  driver = img_dataset.driver\n",
    "  print('Raster driver: {d}\\n'.format(d=driver))\n",
    "\n",
    "  # What is the raster's projection?\n",
    "  proj = img_dataset.crs\n",
    "  print('Image projection:')\n",
    "  print(proj)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, class_dict,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues,model_name='test'):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    # convert class_id to class_name using the class_dict\n",
    "    cover_names = []\n",
    "    for cover_class in classes:\n",
    "        cover_names.append(class_dict[cover_class])\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    else:\n",
    "        pass\n",
    "    #print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=cover_names, yticklabels=cover_names,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    #save a copy of the classification matrix\n",
    "    fig.savefig('classification_matrix_' + model_name + '.png')\n",
    "    return ax\n",
    "\n",
    "#Keras compatible data generator - works with rasterio compatible files (tiff only as written)\n",
    "def generator(samples,batch_size,width,height,layers,class_count):\n",
    "    \"\"\"\n",
    "    Yields next training batch, checks shape of tile, ensure image format - includes DEM\n",
    "    \"\"\"\n",
    "    num_samples = len(samples)\n",
    "    while True: # Loop forever so the generator never terminates\n",
    "        #shuffle(samples)\n",
    "        # Get index to start each batch: [0, batch_size, 2*batch_size, ..., max multiple of batch_size <= num_samples]\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            # Get the samples you'll use in this batch\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            # Initialise X_train and y_train arrays for this batch\n",
    "            X_train = []\n",
    "            y_train = []\n",
    "\n",
    "            # For each example\n",
    "            for batch_sample in batch_samples:\n",
    "                # Load image (X) and label (y)\n",
    "                img_name = batch_sample[0]\n",
    "                label = batch_sample[1]\n",
    "                #load in file\n",
    "                #print(os.path.join(data_path,img_name))\n",
    "                with rasterio.open(os.path.join(img_name)) as ds:\n",
    "                    tile=ds.read()\n",
    "                #perform any preprocessing required\n",
    "                tile,label = preprocessing(tile,label,class_count,layers,width,height)     \n",
    "\n",
    "                # Add example to arrays\n",
    "                X_train.append(tile)\n",
    "                y_train.append(label)\n",
    "\n",
    "            # Make sure they're numpy arrays (as opposed to lists)\n",
    "            X_train = np.array(X_train)\n",
    "            #X_train = np.asarray(X_train).astype('float32')\n",
    "            y_train = np.array(y_train)\n",
    "\n",
    "            # The generator-y part: yield the next training batch            \n",
    "            yield X_train, y_train\n",
    "\n",
    "#ensure size of tile is uniform\n",
    "def preprocessing(tile,label,class_count,layers,width,height):\n",
    "    #print(tile.shape,\"before\")\n",
    "    #to avoid artifacts, no antialiasing whn rescaling\n",
    "    tile = resize(tile, (layers,width,height),anti_aliasing=False)\n",
    "    #print(tile.shape,\"resize\")\n",
    "    #Returns the source array reshaped into the order expected by image processing and visualization software (matplotlib, scikit-image, etc) by swapping the axes order from (bands, rows, columns) to (rows, columns, bands)\n",
    "    tile = reshape_as_image(tile)\n",
    "    #print(tile.shape,\"reshape\")\n",
    "    \n",
    "    #normalising tile\n",
    "    tile = tile/255\n",
    "    \n",
    "    label = to_categorical(label,class_count)\n",
    "    return tile,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a347f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 83, 83, 64)        4096      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 83, 83, 64)       256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 83, 83, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 41, 41, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 107584)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               27541760  \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,547,650\n",
      "Trainable params: 27,547,010\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "1555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gavin\\.conda\\envs\\Visualistion_Rasterio_GPU\\lib\\site-packages\\ipykernel_launcher.py:32: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 18s 294ms/step\n",
      "Accuracy: 0.997419\n",
      "Precision: 0.997424\n",
      "Recall: 0.997418\n",
      "F1 score: 0.997419\n",
      "Cohens kappa: 0.994839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Confusion matrix, without normalization'}, xlabel='Predicted label', ylabel='True label'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Model config\n",
    "width = 83 #32#83  --16\n",
    "height= 83 #32#83  --16\n",
    "layers = 7 #7 sat #6 drone\n",
    "input_shape = (width, height, layers)\n",
    "class_count = 2 #5\n",
    "batch = 25 #25\n",
    "epoch = 200\n",
    "model_name =  'Drone_Tuned_BasicCNN'\n",
    "\n",
    "class_names = dict((\n",
    "(0,  'noprospect'),\n",
    "(1, 'prospect'),\n",
    "))\n",
    "\n",
    "model = tf.keras.models.load_model(r\"C:\\Users\\Gavin\\Models\\Thesis Models 2022\\Drone\\CNNBase\\tuner\\BasicCNN_FullDrone_tuned\")\n",
    "model.summary()\n",
    "\n",
    "fulldataset_df = load_samples('BinFullDrone_dataset.csv')\n",
    "gridsearch_df = pd.DataFrame(columns=['Modelname','Epoch','Batch_size','Accuracy','Precision','Recall','F1_Score','Cohens_kappa'])\n",
    "\n",
    "train_df,test_df = train_test_split(fulldataset_df, test_size=0.2) #0.2\n",
    "test_df,valid_df = train_test_split(test_df, test_size=0.5)\n",
    "test_count = len(test_df)\n",
    "print(test_count)\n",
    "\n",
    "test_datagen = generator(samples=test_df,batch_size=batch_size,width=width,height=height,layers=layers,class_count=class_count)\n",
    "\n",
    "#predictions against test dataset\n",
    "predictions = model.predict_generator(generator=test_datagen, \n",
    "                        steps= test_count // batch,\n",
    "                         verbose=1)\n",
    "\n",
    "eval_generator = generator(samples=test_df,batch_size=1,width=width,height=height,layers=layers,class_count=class_count)\n",
    "#determining labels\n",
    "labels = np.empty(predictions.shape)\n",
    "count = 0\n",
    "while count < len(labels):\n",
    "    image_b, label_b = next(eval_generator)\n",
    "    labels[count] = label_b\n",
    "    count += 1\n",
    "\n",
    "label_index = np.argmax(labels, axis=1)     \n",
    "pred_index = np.argmax(predictions, axis=1)\n",
    "#calcualte metrics\n",
    "accuracy = accuracy_score(label_index, pred_index)\n",
    "precision = precision_score(label_index, pred_index,average='macro')\n",
    "recall = recall_score(label_index, pred_index, average='macro')\n",
    "f1 = f1_score(label_index, pred_index,average='macro')\n",
    "kappa = cohen_kappa_score(label_index, pred_index)\n",
    "gridsearch_df=gridsearch_df.append({'Modelname':model_name,'Epoch':epoch,'Batch_size':batch,'Accuracy':accuracy,'Precision':precision,'Recall':recall,'F1_Score':f1,'Cohens_kappa':kappa},ignore_index=True)\n",
    "\n",
    "#create a classification report and save a copy\n",
    "clsf_report = pd.DataFrame(classification_report(y_true = label_index, y_pred = pred_index, output_dict=True)).transpose()\n",
    "clsf_report.to_csv('ClassificationReport_'+ model_name + '.csv', index= True)\n",
    "\n",
    "print('Accuracy: %f' % accuracy)\n",
    "print('Precision: %f' % precision)\n",
    "print('Recall: %f' % recall)\n",
    "print('F1 score: %f' % f1)\n",
    "print('Cohens kappa: %f' % kappa)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(label_index, pred_index, classes=np.array(list(class_names)),\n",
    "                          class_dict=class_names,model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79239afb-5218-4f18-ad9e-e7e976ac85d5",
   "metadata": {},
   "source": [
    "output_filename = 'basicCNN_sat16_bestmodel'\n",
    "dir_name = './BasicCNN_Sat_16_5'\n",
    "import shutil\n",
    "shutil.make_archive(output_filename, 'zip', dir_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798420b6-30bb-426b-88a6-85bbc7071f97",
   "metadata": {},
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c34d94-7e61-414b-a1cb-7125cba08dbe",
   "metadata": {},
   "source": [
    "#code to check the effects of upscaling the image - seems fine\n",
    "width = 32 #32#83  --16\n",
    "height= 32 #32#83  --16\n",
    "layers = 7 #7 #6 drone\n",
    "\n",
    "#check that a image tile opens \n",
    "img_filename = './SatBinDataset/noprospect/noprospect77_F.tif'\n",
    "with rasterio.open(img_filename) as ds:\n",
    "    tile = ds.read()\n",
    "    tile = resize(tile, (layers,width,height),anti_aliasing=False)\n",
    "    #tile = reshape_as_image(tile)\n",
    "\n",
    "with rasterio.open('test3.tif',\n",
    "    'w',\n",
    "    driver='GTiff',\n",
    "    height=tile.shape[1],\n",
    "    width=tile.shape[2],\n",
    "    count=tile.shape[0],\n",
    "    dtype=tile.dtype,\n",
    "    tile=ds.crs,\n",
    "    nodata=None,\n",
    "    transform=ds.transform\n",
    "    ) as dst:\n",
    "    dst.write(tile)\n",
    "    \n",
    "img_filename = 'test.tif'\n",
    "\n",
    "tile = rasterio.open(img_filename)\n",
    "plt.imshow(tile.read(7), cmap='pink')\n",
    "plt.show()\n",
    "\n",
    "print(tile.shape)\n",
    "\n",
    "img_filename = 'test2.tif'\n",
    "\n",
    "tile = rasterio.open(img_filename)\n",
    "plt.imshow(tile.read(7), cmap='pink')\n",
    "plt.show()\n",
    "\n",
    "print(tile.shape)\n",
    "\n",
    "img_filename = 'test3.tif'\n",
    "\n",
    "tile = rasterio.open(img_filename)\n",
    "plt.imshow(tile.read(7), cmap='pink')\n",
    "plt.show()\n",
    "\n",
    "print(tile.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74212713-95e0-4314-a970-7a0d70b10879",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
