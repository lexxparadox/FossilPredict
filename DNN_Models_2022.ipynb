{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d6e2b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten,Conv2D, MaxPooling2D, AveragePooling2D,Activation, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "#from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "import rasterio\n",
    "from rasterio.plot import reshape_as_image\n",
    "\n",
    "#seed set to allow for reproducibility within the results\n",
    "seed = 2505\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9cce5c",
   "metadata": {},
   "source": [
    "General Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d92256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple CSV loader\n",
    "def load_samples(csv_file):\n",
    "    data = pd.read_csv(os.path.join(csv_file))\n",
    "    data = data[['FileName','Label','ClassName']]\n",
    "    file_names = list(data.iloc[:,0])\n",
    "    #Get labels withing second column\n",
    "    labels = list(data.iloc[:,1])\n",
    "    samples=[]\n",
    "    for samp,lab in zip(file_names,labels):\n",
    "        samples.append([samp,lab])\n",
    "    return samples\n",
    "\n",
    "#helper function to report  a images details\n",
    "def image_details(image):\n",
    "  num_bands = img_dataset.count\n",
    "  print('Number of bands in image: {n}\\n'.format(n=num_bands))\n",
    "\n",
    "  # How many rows and columns?\n",
    "  rows, cols = img_dataset.shape\n",
    "  print('Image size is: {r} rows x {c} columns\\n'.format(r=rows, c=cols))\n",
    "\n",
    "  # What driver was used to open the raster?\n",
    "  driver = img_dataset.driver\n",
    "  print('Raster driver: {d}\\n'.format(d=driver))\n",
    "\n",
    "  # What is the raster's projection?\n",
    "  proj = img_dataset.crs\n",
    "  print('Image projection:')\n",
    "  print(proj)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, class_dict,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues,model_name='test'):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    # convert class_id to class_name using the class_dict\n",
    "    cover_names = []\n",
    "    for cover_class in classes:\n",
    "        cover_names.append(class_dict[cover_class])\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    else:\n",
    "        pass\n",
    "    #print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=cover_names, yticklabels=cover_names,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    #save a copy of the classification matrix\n",
    "    fig.savefig('classification_matrix_' + model_name + '.png')\n",
    "    return ax\n",
    "\n",
    "def save_metrics(gridsearch_df,test_datagen,test_df,model_name,model,width,height,layers,class_count,test_count,batch,epoch):\n",
    "    #predictions against test dataset\n",
    "    predictions = model.predict_generator(generator=test_datagen, \n",
    "                            steps= test_count // batch,\n",
    "                             verbose=1)\n",
    "\n",
    "    eval_generator = generator(samples=test_df,batch_size=1,width=width,height=height,layers=layers,class_count=class_count)\n",
    "    #determining labels\n",
    "    labels = np.empty(predictions.shape)\n",
    "    count = 0\n",
    "    while count < len(labels):\n",
    "        image_b, label_b = next(eval_generator)\n",
    "        labels[count] = label_b\n",
    "        count += 1\n",
    "\n",
    "    label_index = np.argmax(labels, axis=1)     \n",
    "    pred_index = np.argmax(predictions, axis=1)\n",
    "    #calcualte metrics\n",
    "    accuracy = accuracy_score(label_index, pred_index)\n",
    "    precision = precision_score(label_index, pred_index,average='macro')\n",
    "    recall = recall_score(label_index, pred_index, average='macro')\n",
    "    f1 = f1_score(label_index, pred_index,average='macro')\n",
    "    kappa = cohen_kappa_score(label_index, pred_index)\n",
    "    gridsearch_df=gridsearch_df.append({'Modelname':model_name,'Epoch':epoch,'Batch_size':batch,'Accuracy':accuracy,'Precision':precision,'Recall':recall,'F1_Score':f1,'Cohens_kappa':kappa},ignore_index=True)\n",
    "    #create a classification report and save a copy\n",
    "    clsf_report = pd.DataFrame(classification_report(y_true = label_index, y_pred = pred_index, output_dict=True)).transpose()\n",
    "    clsf_report.to_csv('ClassificationReport_'+ model_name + '.csv', index= True)\n",
    "    \n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    print('Precision: %f' % precision)\n",
    "    print('Recall: %f' % recall)\n",
    "    print('F1 score: %f' % f1)\n",
    "    print('Cohens kappa: %f' % kappa)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plot_confusion_matrix(label_index, pred_index, classes=np.array(list(class_names)),\n",
    "                              class_dict=class_names,model_name=model_name)\n",
    "    \n",
    "    return gridsearch_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3ae586",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e68c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure size of tile is uniform\n",
    "def preprocessing(tile,label,class_count,layers,width,height):\n",
    "    #print(tile.shape,\"before\")\n",
    "    #to avoid artifacts, no antialiasing whn rescaling\n",
    "    tile = resize(tile, (layers,width,height),anti_aliasing=False)\n",
    "    #print(tile.shape,\"resize\")\n",
    "    #Returns the source array reshaped into the order expected by image processing and visualization software (matplotlib, scikit-image, etc) by swapping the axes order from (bands, rows, columns) to (rows, columns, bands)\n",
    "    tile = reshape_as_image(tile)\n",
    "    #print(tile.shape,\"reshape\")\n",
    "    \n",
    "    #normalising tile\n",
    "    tile = tile/255\n",
    "    \n",
    "    label = to_categorical(label,class_count)\n",
    "    return tile,label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c64b1",
   "metadata": {},
   "source": [
    "Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07f113e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras compatible data generator - works with rasterio compatible files (tiff only as written)\n",
    "def generator(samples,batch_size,width,height,layers,class_count):\n",
    "    \"\"\"\n",
    "    Yields next training batch, checks shape of tile, ensure image format - includes DEM\n",
    "    \"\"\"\n",
    "    num_samples = len(samples)\n",
    "    while True: # Loop forever so the generator never terminates\n",
    "        #shuffle(samples)\n",
    "        # Get index to start each batch: [0, batch_size, 2*batch_size, ..., max multiple of batch_size <= num_samples]\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            # Get the samples you'll use in this batch\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            # Initialise X_train and y_train arrays for this batch\n",
    "            X_train = []\n",
    "            y_train = []\n",
    "\n",
    "            # For each example\n",
    "            for batch_sample in batch_samples:\n",
    "                # Load image (X) and label (y)\n",
    "                img_name = batch_sample[0]\n",
    "                label = batch_sample[1]\n",
    "                #load in file\n",
    "                #print(os.path.join(data_path,img_name))\n",
    "                with rasterio.open(os.path.join(img_name)) as ds:\n",
    "                    tile=ds.read()\n",
    "                #perform any preprocessing required\n",
    "                tile,label = preprocessing(tile,label,class_count,layers,width,height)     \n",
    "\n",
    "                # Add example to arrays\n",
    "                X_train.append(tile)\n",
    "                y_train.append(label)\n",
    "\n",
    "            # Make sure they're numpy arrays (as opposed to lists)\n",
    "            X_train = np.array(X_train)\n",
    "            #X_train = np.asarray(X_train).astype('float32')\n",
    "            y_train = np.array(y_train)\n",
    "\n",
    "            # The generator-y part: yield the next training batch            \n",
    "            yield X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a838212e",
   "metadata": {},
   "source": [
    "Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "930c2997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Basic_CNN(input_dropout_rate,hidden_dropout_rate,input_shape,optimizer):\n",
    "    #drop1 = dropout_rate, drop2 = dropout_rate_in\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(input_dropout_rate)) #0.25\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(192))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(hidden_dropout_rate)) #0.25\n",
    "\n",
    "    model.add(Dense(class_count))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    #model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=metrics)\n",
    "    #opt = tensorflow.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    #plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47d74d62-2b3b-487d-93fd-c675b1a8fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Basic_CNN_experiment(input_dropout_rate,hidden_dropout_rate,input_shape,optimizer):\n",
    "    #drop1 = dropout_rate, drop2 = dropout_rate_in\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('LeakyReLU'))#relu\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('LeakyReLU'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(input_dropout_rate)) #0.25\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('LeakyReLU'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(input_dropout_rate)) #0.25\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('LeakyReLU'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(input_dropout_rate)) #0.25\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('LeakyReLU'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(input_dropout_rate)) #0.25\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(hidden_dropout_rate)) #0.25\n",
    "\n",
    "    model.add(Dense(class_count))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    #model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=metrics)\n",
    "    #opt = tensorflow.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    #plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dec00511-fb22-43f7-95c8-8bb966499001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Basic_CNN_bottleneck1(input_dropout_rate,hidden_dropout_rate,input_shape,optimizer):\n",
    "    #drop1 = dropout_rate, drop2 = dropout_rate_in\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('LeakyReLU'))#relu\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('LeakyReLU'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(input_dropout_rate)) #0.25\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('LeakyReLU'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(input_dropout_rate)) #0.25\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(hidden_dropout_rate)) #0.25\n",
    "\n",
    "    model.add(Dense(class_count))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    #model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=metrics)\n",
    "    #opt = tensorflow.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    #plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19429bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(dropout_rate,input_shape,optimizer):\n",
    "    #drop1 = dropout_rate, drop2 = dropout_rate_in\n",
    "    base_model = applications.resnet50.ResNet50(weights= None, include_top=False, input_shape= input_shape)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    predictions = Dense(class_count, activation= 'softmax')(x)\n",
    "    model = Model(inputs = base_model.input, outputs = predictions)\n",
    "    #model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=metrics)\n",
    "    #opt = tensorflow.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['acc'])\n",
    "    print(model.summary())\n",
    "    print(model.metrics_names)\n",
    "    #plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfbace23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG16(dropout_rate,input_shape,optimizer):\n",
    "    #drop1 = dropout_rate, drop2 = dropout_rate_in\n",
    "    base_model = applications.vgg16.VGG16(weights= None, include_top=False, input_shape= input_shape)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    predictions = Dense(class_count, activation= 'softmax')(x)\n",
    "    model = Model(inputs = base_model.input, outputs = predictions)\n",
    "    #model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=metrics)\n",
    "    #opt = tensorflow.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['acc'])\n",
    "    #print(model.summary())\n",
    "    #print(model.metrics_names)\n",
    "    #plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db30812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Xception(dropout_rate,input_shape,optimizer):\n",
    "    #drop1 = dropout_rate, drop2 = dropout_rate_in\n",
    "    base_model = applications.Xception(weights=None, include_top=False, input_shape= input_shape)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    predictions = Dense(class_count, activation= 'softmax')(x)\n",
    "    model = Model(inputs = base_model.input, outputs = predictions)\n",
    "    #model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=metrics)\n",
    "    #opt = tensorflow.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['acc'])\n",
    "    print(model.summary())\n",
    "    print(model.metrics_names)\n",
    "    #plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "574040ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EfficientNet(dropout_rate,input_shape,optimizer):\n",
    "    #drop1 = dropout_rate, drop2 = dropout_rate_in\n",
    "    base_model = applications.efficientnet.EfficientNetB7(include_top=False,weights=None, input_shape= input_shape)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    predictions = Dense(class_count, activation= 'softmax')(x)\n",
    "    model = Model(inputs = base_model.input, outputs = predictions)\n",
    "    #model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=metrics)\n",
    "    #opt = tensorflow.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['acc'])\n",
    "    #print(model.summary())\n",
    "    #print(model.metrics_names)\n",
    "    #plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c71e4ccc-26e3-451c-92ac-2c8e6808353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gullydetector_CNN(input_shape,class_count,units,activation,dropout,lr):\n",
    "    #drop1 = dropout_rate, drop2 = dropout_rate_in\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation = activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout)) #0.25\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units = units))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation = activation))\n",
    "    model.add(Dropout(dropout)) #0.25\n",
    "\n",
    "    model.add(Dense(class_count))\n",
    "    model.add(Activation(activation='softmax'))\n",
    "\n",
    "    #model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=metrics)\n",
    "    #opt = tensorflow.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "                  metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    #plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfd1165",
   "metadata": {},
   "source": [
    "Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f15d1937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the data list is:  ['donga', 'dried_mudflat', 'shrub_vegetation', 'sandstone', 'grass_vegetation']\n"
     ]
    }
   ],
   "source": [
    "#dictionary necessary to identify classes for confusion matrix\n",
    "#class_names = dict((\n",
    "#(0,  'noprospect'),\n",
    "#(1, 'prospect'),\n",
    "#))\n",
    "\n",
    "class_names = dict((\n",
    "(0,  'donga'),\n",
    "(1, 'dried_mudflat'),\n",
    "(2, 'grass_vegetation'),\n",
    "(3, 'sandstone'),\n",
    "(4, 'shrub_vegetation'),\n",
    "))\n",
    "\n",
    "#Model config\n",
    "width = 83 #32#83  --16\n",
    "height= 83 #32#83  --16\n",
    "layers = 7 #7 sat #7 drone\n",
    "batch_size = 25 #25\n",
    "epochs = 100 #50\n",
    "model_name =  'LULC_gullydetector'\n",
    "verbose_setting = 1\n",
    "count = 1\n",
    "CV_split = 10\n",
    "\n",
    "#hyperparameter settings\n",
    "input_shape = (width, height, layers)\n",
    "units = 448\n",
    "activation = \"relu\"\n",
    "dropout = 0.6\n",
    "lr = 1e-05\n",
    "class_count = 5\n",
    "#declare all needed dataframes\n",
    "gridsearch_df = pd.DataFrame(columns=['Modelname','Epoch','Batch_size','Accuracy','Precision','Recall','F1_Score','Cohens_kappa'])\n",
    "data_path = './Dataset' #BinFullDroneDataset32\n",
    "data_dir_list = os.listdir(data_path)\n",
    "print ('the data list is: ',data_dir_list)\n",
    "fulldataset_df = load_samples('Full_drone_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fee05c6",
   "metadata": {},
   "source": [
    "Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c490138b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_count=3658\n",
      "train_count=32918\n",
      "test_count=4064\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 49\u001b[0m\n\u001b[1;32m     45\u001b[0m valid_datagen \u001b[38;5;241m=\u001b[39m generator(samples\u001b[38;5;241m=\u001b[39mvalid_df,batch_size\u001b[38;5;241m=\u001b[39mbatch_size,width\u001b[38;5;241m=\u001b[39mwidth,height\u001b[38;5;241m=\u001b[39mheight,layers\u001b[38;5;241m=\u001b[39mlayers,class_count\u001b[38;5;241m=\u001b[39mclass_count)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m#create and laod model\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#model = EfficientNet(dropout_rate,input_shape,optimizer)\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGullydetector_CNN\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43munits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mclass_count\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclass_count\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m#model = Basic_CNN(dropout_rate,dropout_rate2,input_shape,optimizer)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#model = Basic_CNN_bottleneck1(dropout_rate,dropout_rate2,input_shape,optimizer)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#create_model_options(input_dropout_rate,hidden_dropout_rate,input_shape,optimizer)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_datagen,steps_per_epoch\u001b[38;5;241m=\u001b[39mtrain_count \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size, verbose\u001b[38;5;241m=\u001b[39mverbose_setting, epochs\u001b[38;5;241m=\u001b[39mepochs,validation_data\u001b[38;5;241m=\u001b[39mvalid_datagen,validation_steps\u001b[38;5;241m=\u001b[39mvalid_count \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size,callbacks\u001b[38;5;241m=\u001b[39mcallbacks)\n",
      "Cell \u001b[0;32mIn [12], line 23\u001b[0m, in \u001b[0;36mGullydetector_CNN\u001b[0;34m(input_shape, class_count, units, activation, dropout, lr)\u001b[0m\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Activation(activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=metrics)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#opt = tensorflow.keras.optimizers.Adam(learning_rate=0.0001)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m---> 23\u001b[0m               optimizer\u001b[38;5;241m=\u001b[39m\u001b[43mkeras\u001b[49m\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlr),\n\u001b[1;32m     24\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#print(model.summary())\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "#train_df,test_df = train_test_split(fulldataset_df, test_size=0.2) #0.2\n",
    "#test_df,valid_df = train_test_split(test_df, test_size=0.5)\n",
    "#use 10% of dataset for test dataset, 10% for validation and 80% for training\n",
    "fulldataset_df,test_df = train_test_split(fulldataset_df, test_size=0.1) #0.2\n",
    "#test_df,valid_df = train_test_split(test_df, test_size=0.5)\n",
    "\n",
    "Y = np.array(fulldataset_df)\n",
    "X = Y[:, 0]\n",
    "Y= Y[:, 1]\n",
    "skf = StratifiedKFold(n_splits = CV_split) #DONT shuffle: https://stackoverflow.com/questions/69100728/how-is-it-that-the-accuracy-score-for-10-fold-cross-validation-is-worst-than-for \n",
    "\n",
    "#optimizer = Ftrl(lr=learn_rates)\n",
    "#optimizer = SGD(lr=learn_rate,momentum=momentum)\n",
    "#optimizer = Adam(learning_rate=learn_rate, epsilon=1e-05) #1.0 or 0.1\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=10, restore_best_weights = True, monitor='val_loss', verbose=verbose_setting),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=verbose_setting),\n",
    "    ModelCheckpoint('model-inprogress-multi.h5', verbose=verbose_setting, save_best_only=True, save_weights_only=True)\n",
    "]\n",
    "\n",
    "for train_index, val_index in skf.split(X,Y):\n",
    "\n",
    "    fulldataset_df = np.array(fulldataset_df)\n",
    "    train_df = fulldataset_df[train_index]\n",
    "    valid_df = fulldataset_df[val_index]\n",
    "    \n",
    "    #counts necessary\n",
    "    valid_count = len(valid_df)\n",
    "    train_count = len(train_df)\n",
    "    test_count = len(test_df)\n",
    "    \n",
    "    print('valid_count='+str(valid_count))\n",
    "    print('train_count='+str(train_count))\n",
    "    print('test_count='+str(test_count))\n",
    "    \n",
    "    smodel_name =  model_name +\"_\" + str(count)\n",
    "    count += 1\n",
    "    input_shape = (width, height, layers)\n",
    "    \n",
    "    #essential to create a fresh model every time\n",
    "    #Create the generators \n",
    "    train_datagen = generator(samples=train_df,batch_size=batch_size,width=width,height=height,layers=layers,class_count=class_count)\n",
    "    test_datagen = generator(samples=test_df,batch_size=batch_size,width=width,height=height,layers=layers,class_count=class_count)\n",
    "    valid_datagen = generator(samples=valid_df,batch_size=batch_size,width=width,height=height,layers=layers,class_count=class_count)\n",
    "\n",
    "    #create and laod model\n",
    "    #model = EfficientNet(dropout_rate,input_shape,optimizer)\n",
    "    model = Gullydetector_CNN(\n",
    "        input_shape =input_shape,units=units, activation=activation, dropout=dropout, lr=lr,class_count = class_count\n",
    "    )\n",
    "    #model = Basic_CNN(dropout_rate,dropout_rate2,input_shape,optimizer)\n",
    "    #model = Basic_CNN_bottleneck1(dropout_rate,dropout_rate2,input_shape,optimizer)\n",
    "\n",
    "    #create_model_options(input_dropout_rate,hidden_dropout_rate,input_shape,optimizer)\n",
    "    history = model.fit(train_datagen,steps_per_epoch=train_count // batch_size, verbose=verbose_setting, epochs=epochs,validation_data=valid_datagen,validation_steps=valid_count // batch_size,callbacks=callbacks)\n",
    "    #steps_per_epoch: Total number of steps (batches of samples) to yield from generator before declaring one epoch finished and starting the next epoch. It should typically be equal to the number of unique samples of your dataset divided by the batch size.\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    #plt.plot(history.history['acc']) #anything but sequential models\n",
    "    #plt.plot(history.history['val_acc'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train_Accuracy', 'Valid_Accuracy'], loc='upper left')\n",
    "    plt.savefig('accuracy_' + smodel_name + '.png', dpi=300)\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train_Loss', 'Valid_Loss'], loc='upper left')\n",
    "    plt.savefig('loss_' + smodel_name + '.png',dpi=300)\n",
    "    \n",
    "    #save model for later use\n",
    "    model.save(smodel_name)\n",
    "    \n",
    "    #run the metrics and save it to a dataframe for later analysis\n",
    "    gridsearch_df = save_metrics(gridsearch_df,test_datagen,test_df,smodel_name,model,width,height,layers,class_count,test_count,batch_size,epochs)\n",
    "    \n",
    "    #save results\n",
    "    gridsearch_df.to_csv('Total_Results_'+ smodel_name + '.csv', index= True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24171af5-f10d-472a-bbbb-da00d302f5f1",
   "metadata": {},
   "source": [
    "output_filename = 'StudyModel32_4'\n",
    "dir_name = './StudyModel32_4'\n",
    "import shutil\n",
    "shutil.make_archive(output_filename, 'zip', dir_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f4f702-d077-4ec6-a9a1-140b5b151fd7",
   "metadata": {},
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('DroneDataset32.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('BinFullDroneDataset32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74212713-95e0-4314-a970-7a0d70b10879",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
